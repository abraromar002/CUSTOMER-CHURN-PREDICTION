# -*- coding: utf-8 -*-
"""PREDICTION CHURN ^-^.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Opq4Wy0P3KNkJGD9_ExADG48VHDjLvl0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

customer= pd.read_csv('/content/Telco_Customer_Churn.csv')

customer.describe()

customer.info()

customer.head(7)

customer.isnull().sum()

# drop unnecessary  column
customer = customer.drop(columns=["customerID"])

#print unique values

for col in customer.columns:
    print(col, customer[col].unique())
    print("="*30)

customer["TotalCharges"] = customer["TotalCharges"].replace([" ", ""], "0.0")

customer["TotalCharges"] = customer["TotalCharges"].astype(float)

"""# **EDA**"""

customer.describe()

customer.head(3)

"""**#Distiribution of numerical columns**"""

def plot_histogram(customer, column_name,color="pink"):

  plt.figure(figsize=(5, 3))
  sns.histplot(customer[column_name], kde=True ,color=color)
  plt.title(f"Distribution of {column_name}")
  plt.legend()
  plt.show()

plot_histogram(customer, "TotalCharges",color="pink")

plot_histogram(customer, "tenure",color="pink")

plot_histogram(customer, "MonthlyCharges",color="pink" )

plt.figure(figsize=(10, 5))
sns.heatmap(customer[["tenure", "MonthlyCharges", "TotalCharges"]].corr(),
            annot=True, cmap="viridis", fmt=".3f", linewidths=1, linecolor="white")
plt.title(" Correlation Heatmap", fontsize=14, fontweight="bold")
plt.xticks(rotation=45)
plt.show()

object_cols = customer.select_dtypes(include="object").columns.to_list()

object_cols = ["SeniorCitizen"] + object_cols

for col in object_cols:
  plt.figure(figsize=(5, 3))
  sns.countplot(x=customer[col])
  plt.title(f"Count Plot of {col}")
  plt.show()

"""# DATA PREPROCESSING"""

customer["Churn"] = customer["Churn"].replace({"Yes": 1, "No": 0})

# Feature Scaling
#scaler = StandardScaler()
#numerical_features = ["tenure", "MonthlyCharges", "TotalCharges"]
#customer[numerical_features] = scaler.fit_transform(customer[numerical_features])

obj_col = customer.select_dtypes(include="object").columns

# label encoding
for column in obj_col:
  label_encoder = LabelEncoder()
  customer[column] = label_encoder.fit_transform(customer[column])

customer.head(4)

"""# SPLIT DATA INTO TRAINING AN TESTING"""

customer.info()

X = customer.drop(columns=["Churn"])
y = customer["Churn"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

print(y_train.value_counts())

"""** I noticed that the value '0' appears more frequently than the value '1', indicating an imbalanced dataset**

# (SMOTE)
"""

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

customer.head()

smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_scaled, y)

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)

random_forest = RandomForestClassifier(random_state=42)
random_forest.fit(X_train, y_train)

y_pred = random_forest.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)

# Predict and evaluate
y_pred = best_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

